---
title: "Tree Models"
author: "Varun Vipul Chodanker"
format: html
editor: visual
---

# Tree Models

This notebook implements random forests, XGBoost and AdaBoost to solve the bankruptcy classification task. There are 4 versions of the training data which each of the models are trained on. These are the original version, the balanced version, the feature selected version and the balanced & feature selected version.

## Setup

First, we clear the environment, load the required libraries and set the configuration parameters.

```{r}

# Reset environment
rm(list=ls())

# Load required libraries
library(caret)
library(readxl)
library(ranger)
library(dplyr)
library(mlr3)
library(pROC)
library(doMC)

# Register the parallel backend to use 7 cores (out of 8)
registerDoMC(7)
```

## Datasets

Each version of the training data, the original test data and the feature selected test data are loaded and prepared for the classification task.

Utility functions are defined below.

```{r}

# Prints outline of the given dataset
desc_bkrpt_data = function(data, name) {
  print("--- Bankruptcy Dataset")
  print(sprintf("Name: %s", name))
  print("Dimensions: ")
  dim(data)
  print(paste0(
    "Proportion that is bankrupt: ", 
    sum(data$Bankrupt == "is_bankrupt")/dim(data)[1]
  ))
  print("Outline: ")
  str(data)
  print("---")
}

# Standardises column names assuming the target is first, and then the features follow
standardise_column_names = function(data) {
  # Target
  names(data)[1] = "y"
  # Features
  names(data)[-1] = paste0('x', 1:(ncol(data)-1))
  
  return(data)
}

# Converts Bankrupt target to categorical for the training of a classification model
prep_class_targ = function(data) {
  data$Bankrupt = as.factor(data$Bankrupt)
  levels(data$Bankrupt) <- c("not_bankrupt", "is_bankrupt")
  return(data)
}

# Filters the given dataset to the given column names
filter_features = function(data, colnames) {
  return(
    data %>%
      select(all_of(colnames))
  )
}

# Reads an excel file to a dataframe
read_xlsx_to_df = function(fp) {
  return(
    data.frame(read_excel(fp))
  )
}
```

All the datasets are loaded in.

```{r}

# Load the training data in
train.data = read_xlsx_to_df("./Datasets/train.xlsx")
# Load the balanced training data in
train_bal.data = read_xlsx_to_df("./Datasets/train_bal.xlsx")
# Load the feature selected training data in
train.feat_sel.data = read_xlsx_to_df("./Datasets/train_feat_sel.xlsx")
# Load the balanced & feature selected training data in
train_bal.feat_sel.data = read_xlsx_to_df("./Datasets/train_bal_feat_sel.xlsx")

# Load the test data in
test.data = read_xlsx_to_df("./Datasets/test.xlsx")
# Feature selected version of the test data
test.feat_sel.data = filter_features(test.data, names(train.feat_sel.data))
```

Verify this.

```{r}

str(train.data)
```

The targets from all the datasets are converted (to categorical) so that they are compatible with classification.

```{r}

# Training datasets...
train.data = prep_class_targ(train.data)
train_bal.data = prep_class_targ(train_bal.data)
train.feat_sel.data = prep_class_targ(train.feat_sel.data)
train_bal.feat_sel.data = prep_class_targ(train_bal.feat_sel.data)
# Test datasets...
test.data = prep_class_targ(test.data)
test.feat_sel.data = prep_class_targ(test.feat_sel.data)
```

Verify this.

```{r}

desc_bkrpt_data(train.data, "Train")
```

## Models

The utility functions related to training and evaluation are defined first. Then, the models are implemented using them.

Utilities related to training. Including those for the weighting of the minority bankrupt class. Notably, the training control selects 5 fold cross validation and ROC metrics.

```{r}

# compared to a weight of 1 for the other non-bankrupt class
get_bankrupt_weight = function (data) {
  return (
    sum(data$Bankrupt == "not_bankrupt") / sum(data$Bankrupt == "is_bankrupt")
  )
}

# weights for each observation based on their class
get_sample_bankrupt_weights = function (data) {
  ws = rep(0, nrow(data))
  not_bankrupt_weight = 1
  is_bankrupt_weight = get_bankrupt_weight(data)
  
  ws[data$Bankrupt == "not_bankrupt"] = not_bankrupt_weight
  ws[data$Bankrupt == "is_bankrupt"] = is_bankrupt_weight
  
  return(ws)
}

# weights for each of the two classes -> to enable cost sensitive learning
get_bankrupt_class_weights = function (data) {
  return (
    c(1, get_bankrupt_weight(data))
  )
}

# Extract the best instance of a model out of a list of many that correspond to different hyperparamter configurations
extract_roc_best_model = function(models) {
  roc_best_idx = which.max( 
    lapply(
      models, 
      (function (candidate) max(candidate$results$ROC)) # based on max ROC
    )
  )
  
  print(paste0("Best Conf: ", names(models)[roc_best_idx]))
  return(
    models[[
      roc_best_idx
    ]]  
  )
}

# Training control for 5 fold cross validation and the sensitivity, specificity and ROC metrics
cv5foldCtrl <- trainControl(method="cv", number=5, classProbs = TRUE, summaryFunction = twoClassSummary, allowParallel = TRUE)
```

ROC evaluation utilities.

```{r}

# Creates the ROC from varying probability thresholds on the (+ve) class probability predictions
create_roc = function(model, eval_data) {
  # (+ve) class probability predictions
  eval_pred = predict(model, eval_data, type="prob")[,2]
  # create ROC of varying sensitivity-specificity trade-offs by varying the threshold
  roc_obj = roc(response = eval_data$Bankrupt, levels=c("not_bankrupt", "is_bankrupt"), predictor=eval_pred)
  
  return(roc_obj)
}

# Plot the ROC with the calculated AUC in the title
evaluate_roc = function(roc_obj, model_name="Unknown") {
  print(
    ggroc(roc_obj, legacy.axes = TRUE) + labs(title = paste0(model_name, ' ROC Curve.\n AUC: ', auc(roc_obj)), x = 'Specificity', y = 'Sensitivity')
  )
}
```

Confusion matrix evaluation utilities.

```{r}

# Confusion matrix of the model based on the default threshold
evaluate_conf_mat = function(model, eval_data) {
  # Make predictons
  eval_pred = NULL
  
  # Ensure just the raw predictions are extracted for further processing
  if (inherits(model, "ranger")) {
    eval_pred = predict(model, data=eval_data)
    eval_pred = eval_pred$predictions
  }
  else {
    eval_pred = predict(model, newdata=eval_data)  
  }
  
  # Report performance with metrics and the confusion matrix
  print(confusionMatrix(eval_pred, eval_data$Bankrupt))
}

# Confusion matrix of the model based on the ROC optimal threshold
evaluate_roc_best_conf_mat = function(model, eval_data, prob_thres) {
  # Make predictions from the given ROC optimal threshold
  eval_pred = as.factor(predict(model, eval_data, type="prob")[,2] > prob_thres)
  levels(eval_pred) = c("not_bankrupt", "is_bankrupt")
  
  # Output the confusion matrix of these predictions
  print(confusionMatrix(eval_pred, eval_data$Bankrupt))
}
```

Utilities for the overall evaluation pipeline.

```{r}

# Retrieves the appropriate test set based on where feature selection is in effect
get_test_set = function(feat_sel) {
  if (feat_sel) {
    return (test.feat_sel.data)
  }
  else {
    return (test.data)
  }
}

# Full evaluation pipeline
apply_eval_pipeline = function(model, feat_sel=FALSE, model_name="Unknown") {
  # Appropriate test set based on feature selection
  eval_data = get_test_set(feat_sel) 
  # Create ROC
  roc_obj = create_roc(model, eval_data)
  # ROC optimised probability threshold (best sensitivity-specificity trade-off)
  roc_best_prob_thres = coords(roc_obj, "best", ret = "threshold")[1, 1]
  
  # Outputs
  print(paste0("-- ", model_name, " --")) # Id
  # Confusion matrix from the ROC optimal threshold
  evaluate_roc_best_conf_mat(model, eval_data, roc_best_prob_thres) 
  evaluate_roc(roc_obj, model_name) # ROC curve + AUC
}
```

### Random Forest

Random forest (`ranger` through `caret`) setup for each scenario of training data. Hyper-parameter tuning optimises the ROC metric.

```{r}

# Hyperparameter tuning grid varies based on whether feature selection is in effect.
get_ranger_grid = function(feat_sel) {
  if (feat_sel) {
    # If it is, there are only 51 features that we can randomly select from (w.r.t mtry)
    return (
      expand.grid(mtry = c(3, 12, 29, 51),
           min.node.size=c(1, 3, 5, 7, 9),
           splitrule=c("gini", "extratrees", "hellinger")
      )
    )
  }
  else {
    # Otherwise, there are at most 91 features
    return (
      expand.grid(mtry = c(3, 12, 29, 47, 73, 91),
           min.node.size=c(1, 3, 5, 7, 9),
           splitrule=c("gini", "extratrees", "hellinger")
      )
    )
  }
}

# Setup for imbalanced datasets - appropriate for the non-balanced training data versions
fit_ranger = function(data, feat_sel=FALSE) {
  rangerGrid = get_ranger_grid(feat_sel)
  
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], method="ranger", 
      metric="ROC", trControl=cv5foldCtrl, tuneGrid=rangerGrid, 
      num.trees=50, max.depth=3, regularization.usedepth = TRUE, 
      weights=get_sample_bankrupt_weights(data), class.weights=get_bankrupt_class_weights(data)
    )
    # Weights for sampling + cost-sensitive learning => combat the imbalance
  )
}

# Setup for balanced datasets
fit_ranger_on_balanced = function(data, feat_sel=FALSE) {
  rangerGrid = get_ranger_grid(feat_sel)
  
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], method="ranger", 
      metric="ROC", trControl=cv5foldCtrl, tuneGrid=rangerGrid, 
      num.trees=50, max.depth=3, regularization.usedepth = TRUE
    )
  )
}
```

Train on each data version.

```{r}

set.seed(0)
# Train the random forest model on the training data
train.fit.rf <- fit_ranger(train.data)
# Train the random forest model on the balanced training data
train_bal.fit.rf <- fit_ranger_on_balanced(train_bal.data)
# Train the random forest model on the feature selected training data
train.feat_sel.fit.rf <- fit_ranger(train.feat_sel.data, feat_sel = TRUE)
# Train the random forest model on the balanced & feature selected training data
train_bal.feat_sel.fit.rf <- fit_ranger_on_balanced(train_bal.feat_sel.data, feat_sel = TRUE)
```

Evaluate each of the fits.

```{r}

apply_eval_pipeline(train.fit.rf, model_name = "Random Forest [Original Data]")
apply_eval_pipeline(train_bal.fit.rf, model_name = "Random Forest [Balanced Data]")
apply_eval_pipeline(train.feat_sel.fit.rf, feat_sel = TRUE, model_name = "Random Forest [Feature Selected Data]")
apply_eval_pipeline(train_bal.feat_sel.fit.rf, feat_sel = TRUE, model_name = "Random Forest [Balanced & Feature Selected Data]")
```

### XGBoost

XGBoost (`xgbTree` through `caret`) setup for each scenario of training data. Hyper-parameter tuning optimises the ROC metric.

```{r}

# Hyperparameter tuning grid
xgbmGrid <-  expand.grid(max_depth = c(3, 6, 9), 
                        nrounds = 100, # number of trees
                        eta = 0.3,
                        gamma = c(5, 15),
                        subsample = 0.65, 
                        colsample_bytree = 0.7, 
                        min_child_weight = c(1, 3)
                        )

# Setup for imbalanced datasets - appropriate for the non-balanced training data versions
fit_xgbm = function(data) {
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], 
      method = "xgbTree", metric="ROC", 
      trControl = cv5foldCtrl, tuneGrid = xgbmGrid, 
      scale_pos_weight=get_bankrupt_weight(data)
      # weight the minority +ve class => combat the class imbalance
    )
  )
}

# Setup for balanced datasets
fit_xgbm_on_balanced = function(data) {
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], 
      method = "xgbTree", metric="ROC", 
      trControl = cv5foldCtrl, tuneGrid = xgbmGrid
    )
  )
}
```

Train on each data version.

```{r}

set.seed(0)

train.fit.xgbm = fit_xgbm(train.data)
train_bal.fit.xgbm = fit_xgbm_on_balanced(train_bal.data)
train.feat_sel.fit.xgbm = fit_xgbm(train.feat_sel.data)
train_bal.feat_sel.fit.xgbm = fit_xgbm_on_balanced(train_bal.feat_sel.data)
```

Evaluate each of the fits.

```{r}

apply_eval_pipeline(train.fit.xgbm, model_name = "XGBoost [Original Data]")
apply_eval_pipeline(train_bal.fit.xgbm, model_name = "XGBoost [Balanced Data]")
apply_eval_pipeline(train.feat_sel.fit.xgbm, feat_sel = TRUE, model_name = "XGBoost [Feature Selected Data]")
apply_eval_pipeline(train_bal.feat_sel.fit.xgbm, feat_sel = TRUE, model_name = "XGBoost [Balanced & Feature Selected Data]")
```

### AdaBoost

AdaBoost (`AdaBag` through `caret`) setup for each scenario of training data. Hyper-parameter tuning optimises the ROC metric.

```{r}

# Hyperparameter tuning grid
adbGrid <-  expand.grid(
                        mfinal= c(50, 100), 
                        maxdepth= c(20, 30)
                        )

# Setup for imbalanced datasets - appropriate for the non-balanced training data versions
fit_adb = function(data) {
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], 
      method = "AdaBag", metric="ROC", 
      trControl = cv5foldCtrl, tuneGrid = adbGrid, 
      weights=get_sample_bankrupt_weights(data)
      # sample weights => combat the class imbalance
    )
  )  
}

# Setup for balanced datasets
fit_adb_on_balanced = function(data) {
  return (
    train(
      x=data[, names(data) != 'Bankrupt'], 
      y=data[["Bankrupt"]], 
      method = "AdaBag", metric="ROC", 
      trControl = cv5foldCtrl, tuneGrid = adbGrid
    )
  )  
}
```

Train on each data version.

```{r}

set.seed(0)

train.fit.adb = fit_adb(train.data)
train_bal.fit.adb = fit_adb_on_balanced(train_bal.data)
train.feat_sel.fit.adb = fit_adb(train.feat_sel.data)
train_bal.feat_sel.fit.adb = fit_adb_on_balanced(train_bal.feat_sel.data)
```

Evaluate each of the fits.

```{r}

apply_eval_pipeline(train.fit.adb, model_name = "AdaBoost [Original Data]")
apply_eval_pipeline(train_bal.fit.adb, model_name = "AdaBoost [Balanced Data]")
apply_eval_pipeline(train.feat_sel.fit.adb, feat_sel = TRUE, model_name = "AdaBoost [Feature Selected Data]")
apply_eval_pipeline(train_bal.feat_sel.fit.adb, feat_sel = TRUE, model_name = "AdaBoost [Balanced & Feature Selected Data]")
```

### Other (Experimentation)

Rotation Forests

```{r}

# Rotation Forests

rotfGrid = expand.grid(
  K = c(30, 91), # number of variables subsets
  L = c(50, 100) # number of trees
)

rotfs <- list()
for (maxdepth in c(3, 9)) {
  current_rotf = train(
  x=train_bal.data[, names(train_bal.data) != 'Bankrupt'], 
  y=train_bal.data[["Bankrupt"]], 
  method = "rotationForest", 
  trControl = cv5foldCtrl, tuneGrid = rotfGrid, metric="ROC", 
  minsplit = 1, maxdepth = maxdepth
  )
  
  rotfs[[paste0(maxdepth, "d")]] = current_rotf
}

train_bal.fit.rotf = extract_roc_best_model(rotfs)
print(train_bal.fit.rotf)
```

```{r}

apply_eval_pipeline(train_bal.fit.rotf, model_name = "Rotation Forest [Balanced Data]")
```

## Variable Importance

The application of decision trees naturally forms variable importance measures based on the appearances in the splitting rules. Thus, we extract variable importances from one of our decision tree models - in this case the XGBoost model that was trained on the balanced dataset.

```{r}

varImp(train_bal.fit.xgbm)
```
